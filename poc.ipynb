{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3b4e7e",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00aff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df49451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        seq_len=256,\n",
    "        max_samples=None,\n",
    "        file_path=\"data/shakespeare.txt\",\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Read Shakespeare text\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "\n",
    "        # Flatten to single sequence, tokeniz and reshape to (num_batches, seq_len) \n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        n_batches = len(tokens) // seq_len\n",
    "        self.sequences = torch.tensor(tokens[:n_batches * seq_len], dtype=torch.long).reshape(n_batches, seq_len)\n",
    "        if max_samples is not None:\n",
    "            self.sequences = self.sequences[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        return {\"input_ids\": seq, \"labels\": seq}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c636a8",
   "metadata": {},
   "source": [
    "### Train GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e140e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338024 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1: 0it [00:00, ?it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val loss: 0.35 | Sample: 'Hello, how are you?claveclaveclaveclaveclave caps caps caps caps caps'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "seq_len = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 1\n",
    "num_samples = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_llm = AutoModelForCausalLM.from_config(AutoConfig.from_pretrained(\"gpt2\"))\n",
    "dataset = ShakespeareDataset(tokenizer, seq_len=256, max_samples=num_samples)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_llm.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        output = model_llm(**batch)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model_llm.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            output = model_llm(**batch)\n",
    "            eval_loss += output.loss.item()\n",
    "    eval_loss /= (len(val_dataloader) * batch_size)\n",
    "\n",
    "    # Log\n",
    "    sample_input_ids = torch.tensor(tokenizer.encode(\"Hello, how are you?\"))\n",
    "    sample_text = tokenizer.decode(model_llm.generate(sample_input_ids.reshape(1, -1), max_length=16)[0])\n",
    "    print(f\"[Epoch {epoch+1}] Val loss: {eval_loss:.2f} | Sample: {repr(sample_text)}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "098e76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sample_input_ids = torch.tensor(tokenizer.encode(\"Hello, how are you?\"))\n",
    "sample_text = tokenizer.decode(model_llm.generate(sample_input_ids.reshape(1, -1), max_length=16)[0])\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model_llm.state_dict(),\n",
    "        \"val_loss\": eval_loss,\n",
    "    },\n",
    "    f\"checkpoints/gpt2-latest-dev.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268c09b",
   "metadata": {},
   "source": [
    "### Train PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e21c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338024 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any' | ids: tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597])\n",
      "' further, hear me speak.\\n\\n' | ids: tensor([2252,   11, 3285,  502, 2740,   13,  198,  198])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Shakespeare dataset\n",
    "file_path = \"../data/shakespeare/main.txt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "seq_len = 8\n",
    "batch_size = 512\n",
    "max_samples = 2\n",
    "\n",
    "dataset = ShakespeareDataset(tokenizer, seq_len=seq_len, max_samples=max_samples)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# visualize\n",
    "for i in range(min(5, max_samples)):\n",
    "    print(f\"{repr(tokenizer.decode(dataset[i]['input_ids']))} | ids: {dataset[i]['input_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ccfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from spflow.meta import Scope\n",
    "from spflow.modules.rat import RatSPN\n",
    "from spflow.modules.leaf import  Binomial, Categorical\n",
    "from spflow import log_likelihood\n",
    "\n",
    "depth = 3\n",
    "n_region_nodes = 5\n",
    "num_leaves = 5\n",
    "num_repetitions = 2\n",
    "n_root_nodes = 1\n",
    "num_feature = seq_len\n",
    "# n = torch.tensor(16) # total count for binomial distribution\n",
    "n = torch.tensor(len(tokenizer))\n",
    "\n",
    "scope = Scope(list(range(0, num_feature)))\n",
    "\n",
    "# rat_leaf_layer = Binomial(scope=scope, n=n, out_channels=num_leaves, num_repetitions=num_repetitions)\n",
    "rat_leaf_layer = Categorical(scope=scope, K=n, out_channels=num_leaves, num_repetitions=num_repetitions)\n",
    "rat = RatSPN(\n",
    "    leaf_modules=[rat_leaf_layer],\n",
    "    n_root_nodes=n_root_nodes,\n",
    "    n_region_nodes=n_region_nodes,\n",
    "    num_repetitions=num_repetitions,\n",
    "    depth=depth,\n",
    "    outer_product=True,\n",
    "    split_halves=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4350104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Loss 86.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 500] Loss 7.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(rat.parameters(), lr=1e-2)\n",
    "n_epochs = 1_000\n",
    "log_every = 500\n",
    "for epoch in range(n_epochs):\n",
    "    for step, batch in tqdm(enumerate(dataloader), leave=False, total=len(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        data = batch['input_ids']\n",
    "        ll = log_likelihood(rat, data)      # (B,)\n",
    "        loss = -ll.mean()                         # NLL\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % log_every == 0:\n",
    "        print(f\"[Epoch {epoch}] Loss {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde47c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen: me speak.\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "from spflow import sample_with_evidence\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "X_tensor = batch['input_ids'].to(torch.float32)\n",
    "\n",
    "# Sample with evidence\n",
    "evidence = X_tensor[0]\n",
    "evidence[:3] = torch.nan\n",
    "evidence = evidence.unsqueeze(0)\n",
    "samples = sample_with_evidence(rat, evidence, is_mpe=True)\n",
    "\n",
    "print(repr(tokenizer.decode(samples[0].to(torch.long))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d36047",
   "metadata": {},
   "source": [
    "### Max decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LogitsProcessorList\n",
    "\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "class HeuristicProcessor(LogitsProcessor):\n",
    "    def __init__(self, model_pc):\n",
    "        super().__init__()\n",
    "        self.model_pc = model_pc\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # does nothing\n",
    "        return scores\n",
    "\n",
    "\n",
    "model_llm_state_dict = torch.load(\"checkpoints/gpt2-latest-dev.pth\")[\"model_state_dict\"]\n",
    "model_llm.load_state_dict(model_llm_state_dict)\n",
    "\n",
    "model_spn_state_dict = torch.load(\"checkpoints/rat-latest-dev.pth\")[\"model_state_dict\"]\n",
    "model_spn.load_state_dict(model_spn_state_dict)\n",
    "\n",
    "model_llm.eval()\n",
    "model_spn.eval()\n",
    "\n",
    "sample_input_ids = torch.tensor(tokenizer.encode(\"Hello, how are you?\"))\n",
    "\n",
    "\n",
    "# Generate text (greedy)\n",
    "greedy_dict = model_llm.generate(sample_input_ids.reshape(1, -1), max_new_tokens=32, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "y_hat_greedy = torch.cat(\n",
    "    (sample_input_ids[1:].reshape(1, -1), greedy_dict['sequences'][:, :-1]),\n",
    "    dim=-1\n",
    ")\n",
    "nll_greedy = torch.cat(greedy_dict['scores'], dim=0).gather(dim=1, index=y_hat_greedy).sum()\n",
    "\n",
    "\n",
    "# Generate text (heuristic)\n",
    "processors = LogitsProcessorList([\n",
    "    HeuristicProcessor(model_spn)\n",
    "])\n",
    "heur_dict = model_llm.generate(\n",
    "    sample_input_ids.reshape(1, -1), \n",
    "    max_new_tokens=32, \n",
    "    do_sample=False, \n",
    "    output_scores=True, \n",
    "    return_dict_in_generate=True,\n",
    "    logits_processor=processors,\n",
    ")\n",
    "y_hat_heur = torch.cat(\n",
    "    (sample_input_ids[1:].reshape(1, -1), greedy_dict['sequences'][:, :-1]),\n",
    "    dim=-1\n",
    ")\n",
    "nll_heur = torch.cat(heur_dict['scores'], dim=0).gather(dim=1, index=y_hat_heur).sum()\n",
    "\n",
    "\n",
    "print(f\"NLL greedy: {nll_greedy.item():.2f}\")\n",
    "print(f\"NLL heuristic: {nll_heur.item():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
