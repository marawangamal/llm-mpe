{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3b4e7e",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00aff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df49451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        seq_len=256,\n",
    "        max_samples=None,\n",
    "        file_path=\"data/shakespeare.txt\",\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Read Shakespeare text\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "\n",
    "        # Flatten to single sequence, tokeniz and reshape to (num_batches, seq_len) \n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        n_batches = len(tokens) // seq_len\n",
    "        self.sequences = torch.tensor(tokens[:n_batches * seq_len], dtype=torch.long).reshape(n_batches, seq_len)\n",
    "        if max_samples is not None:\n",
    "            self.sequences = self.sequences[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        return {\"input_ids\": seq, \"labels\": seq}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c636a8",
   "metadata": {},
   "source": [
    "### Train GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e140e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338024 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1: 0it [00:00, ?it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 86.73 | Val loss: 0.34 | Sample: 'Hello, how are you?alkingalkingalkingalkingalkingalkingalkingalkingalkingalking'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "seq_len = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 1\n",
    "num_samples = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_llm = AutoModelForCausalLM.from_config(AutoConfig.from_pretrained(\"gpt2\"))\n",
    "dataset = ShakespeareDataset(tokenizer, seq_len=256, max_samples=num_samples)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_llm.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        output = model_llm(**batch)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model_llm.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            output = model_llm(**batch)\n",
    "            eval_loss += output.loss.item()\n",
    "    eval_loss /= (len(val_dataloader) * batch_size)\n",
    "\n",
    "    # Log\n",
    "    sample_input_ids = torch.tensor(tokenizer.encode(\"Hello, how are you?\"))\n",
    "    sample_text = tokenizer.decode(model_llm.generate(sample_input_ids.reshape(1, -1), max_length=16)[0])\n",
    "    print(f\"[Epoch {epoch+1}] Train loss: {loss.item():.2f} | Val loss: {eval_loss:.2f} | Sample: {repr(sample_text)}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "098e76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sample_input_ids = torch.tensor(tokenizer.encode(\"Hello, how are you?\"))\n",
    "sample_text = tokenizer.decode(model_llm.generate(sample_input_ids.reshape(1, -1), max_length=16)[0])\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model_llm.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss.item(),\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"sample_text\": sample_text,\n",
    "    },\n",
    "    f\"checkpoints/gpt2-latest-dev.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f6062",
   "metadata": {},
   "source": [
    "### Train PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a9fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338024 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any'\n",
      "' further, hear me speak.\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# # 1. Shakespeare dataset\n",
    "file_path = \"../data/shakespeare/main.txt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "seq_len = 8\n",
    "batch_size = 512\n",
    "max_samples = 10\n",
    "\n",
    "dataset = ShakespeareDataset(tokenizer, seq_len=seq_len, max_samples=max_samples)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# visualize\n",
    "for i in range(min(2, max_samples)):\n",
    "    print(repr(tokenizer.decode(dataset[i]['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75ef4284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 51] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 61] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 71] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 81] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 91] Loss 86.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from spflow.modules.rat import RatSPN\n",
    "from spflow.modules.leaf import Categorical\n",
    "from spflow.meta import Scope\n",
    "from spflow import log_likelihood\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_features = seq_len\n",
    "K = len(tokenizer) \n",
    "batch_size = 256\n",
    "\n",
    "scope = Scope(list(range(num_features)))\n",
    "\n",
    "leaf_layer = Categorical(\n",
    "    scope=scope,\n",
    "    out_channels=4,\n",
    "    num_repetitions=2,\n",
    "    K=K,\n",
    ")\n",
    "\n",
    "model_spn = RatSPN(\n",
    "    leaf_modules=[leaf_layer],\n",
    "    n_root_nodes=1,\n",
    "    n_region_nodes=8,\n",
    "    num_repetitions=2,\n",
    "    depth=3,\n",
    "    outer_product=False,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "n_epochs = 100\n",
    "log_every = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for step, batch in tqdm(enumerate(dataloader), leave=False, total=len(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        data = batch['input_ids']\n",
    "        ll = log_likelihood(model_spn, data)          # (B,)\n",
    "        loss = -ll.mean()                         # NLL\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % log_every == 0:\n",
    "        print(f\"[Epoch {epoch+1}] Loss {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9af818ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model_spn.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss.item(),\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"sample_text\": sample_text,\n",
    "    },\n",
    "    f\"checkpoints/rat-latest-dev.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2065be9",
   "metadata": {},
   "source": [
    "### Max decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45cdd163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL greedy: 66.43\n",
      "NLL heuristic: 66.43\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "\n",
    "\n",
    "model_llm_state_dict = torch.load(\"checkpoints/gpt2-latest-dev.pth\")[\"model_state_dict\"]\n",
    "model_llm.load_state_dict(model_llm_state_dict)\n",
    "\n",
    "model_spn_state_dict = torch.load(\"checkpoints/rat-latest-dev.pth\")[\"model_state_dict\"]\n",
    "model_spn.load_state_dict(model_spn_state_dict)\n",
    "\n",
    "model_llm.eval()\n",
    "model_spn.eval()\n",
    "\n",
    "sample_input_ids = torch.tensor(tokenizer.encode(\"Hello, how are you?\"))\n",
    "\n",
    "\n",
    "# Generate text (greedy)\n",
    "greedy_dict = model_llm.generate(sample_input_ids.reshape(1, -1), max_new_tokens=32, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "y_hat_greedy = torch.cat(\n",
    "    (sample_input_ids[1:].reshape(1, -1), greedy_dict['sequences'][:, :-1]),\n",
    "    dim=-1\n",
    ")\n",
    "nll_greedy = torch.cat(greedy_dict['scores'], dim=0).gather(dim=1, index=y_hat_greedy).sum()\n",
    "\n",
    "\n",
    "# Generate text (heuristic)\n",
    "processors = LogitsProcessorList([\n",
    "    # BanListProcessor(tokenizer, common_words, penalty=100.0)\n",
    "])\n",
    "heur_dict = model_llm.generate(\n",
    "    sample_input_ids.reshape(1, -1), \n",
    "    max_new_tokens=32, \n",
    "    do_sample=False, \n",
    "    output_scores=True, \n",
    "    return_dict_in_generate=True,\n",
    "    logits_processor=processors,\n",
    ")\n",
    "y_hat_heur = torch.cat(\n",
    "    (sample_input_ids[1:].reshape(1, -1), greedy_dict['sequences'][:, :-1]),\n",
    "    dim=-1\n",
    ")\n",
    "nll_heur = torch.cat(heur_dict['scores'], dim=0).gather(dim=1, index=y_hat_heur).sum()\n",
    "\n",
    "\n",
    "print(f\"NLL greedy: {nll_greedy.item():.2f}\")\n",
    "print(f\"NLL heuristic: {nll_heur.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb9d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
